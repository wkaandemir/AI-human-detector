#!/usr/bin/env python3
"""
AI Human Detector - Veri Seti HazÄ±rlama Scripti

Bu script, indirilen verisetlerini train/val/test olarak bÃ¶ler ve
data augmentation uygular.

KullanÄ±m:
    python scripts/prepare_dataset.py --input ./data/datasets --output ./data/processed
"""

import os
import sys
import argparse
import shutil
from pathlib import Path
from typing import Tuple, List
import logging
import random

# Logging ayarlarÄ±
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def check_dependencies() -> bool:
    """
    Gerekli kÃ¼tÃ¼phanelerin kurulu olup olmadÄ±ÄŸÄ±nÄ± kontrol eder.
    """
    try:
        import PIL
        import numpy as np
        logger.info("âœ… TÃ¼m baÄŸÄ±mlÄ±lÄ±klar kurulu")
        return True
    except ImportError as e:
        logger.error(f"âŒ Eksik baÄŸÄ±mlÄ±lÄ±k: {e}")
        logger.info("Kurulum iÃ§in: pip install pillow numpy")
        return False


def split_dataset(
    input_dir: Path,
    output_dir: Path,
    train_ratio: float = 0.8,
    val_ratio: float = 0.1,
    test_ratio: float = 0.1,
    seed: int = 42
) -> Tuple[Path, Path, Path]:
    """
    Verisetini train/val/test olarak bÃ¶ler.

    Args:
        input_dir: Girdi dizini (real/fake klasÃ¶rleri)
        output_dir: Ã‡Ä±ktÄ± dizini
        train_ratio: EÄŸitim oranÄ± (varsayÄ±lan: 0.8)
        val_ratio: DoÄŸrulama oranÄ± (varsayÄ±lan: 0.1)
        test_ratio: Test oranÄ± (varsayÄ±lan: 0.1)
        seed: Rastgelelik tohumu

    Returns:
        (train_dir, val_dir, test_dir) demeti
    """
    import numpy as np
    from PIL import Image

    # OranlarÄ±n toplamÄ± 1 olmalÄ±
    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \
        "OranlarÄ±n toplamÄ± 1.0 olmalÄ±"

    logger.info("ğŸ“Š Veriseti bÃ¶lÃ¼nÃ¼yor...")

    # Ã‡Ä±ktÄ± dizinlerini oluÅŸtur
    splits = {
        "train": train_ratio,
        "val": val_ratio,
        "test": test_ratio
    }

    split_dirs = {}
    for split_name in splits.keys():
        split_dirs[split_name] = {
            "real": output_dir / split_name / "real",
            "fake": output_dir / split_name / "fake"
        }
        for label_dir in split_dirs[split_name].values():
            label_dir.mkdir(parents=True, exist_ok=True)

    # Her etiket iÃ§in (real/fake)
    for label in ["real", "fake"]:
        label_input_dir = input_dir / label

        if not label_input_dir.exists():
            logger.warning(f"âš ï¸ Dizin bulunamadÄ±: {label_input_dir}")
            continue

        # GÃ¶rselleri bul
        image_files = []
        for ext in ["*.png", "*.jpg", "*.jpeg", "*.webp"]:
            image_files.extend(label_input_dir.glob(ext))

        if not image_files:
            logger.warning(f"âš ï¸ GÃ¶rsel bulunamadÄ±: {label_input_dir}")
            continue

        logger.info(f"ğŸ“ {label.upper()}: {len(image_files)} gÃ¶rsel bulundu")

        # KarÄ±ÅŸtÄ±r
        random.seed(seed)
        np.random.seed(seed)
        random.shuffle(image_files)

        # BÃ¶l
        n_total = len(image_files)
        n_train = int(n_total * train_ratio)
        n_val = int(n_total * val_ratio)

        train_files = image_files[:n_train]
        val_files = image_files[n_train:n_train + n_val]
        test_files = image_files[n_train + n_val:]

        # Kopyala
        for split_name, files in [("train", train_files), ("val", val_files), ("test", test_files)]:
            output_split_dir = split_dirs[split_name][label]
            for img_file in files:
                # GÃ¶rseli oku ve doÄŸrula
                try:
                    img = Image.open(img_file)
                    img.verify()

                    # Yeniden aÃ§ (verify kapÄ±yor)
                    img = Image.open(img_file)

                    # RGB'ye Ã§evir (gerekirse)
                    if img.mode != 'RGB':
                        img = img.convert('RGB')

                    # Kaydet
                    output_path = output_split_dir / img_file.name
                    img.save(output_path, quality=95)

                except Exception as e:
                    logger.warning(f"âš ï¸ GÃ¶rsel atlandÄ± {img_file.name}: {e}")
                    continue

        logger.info(f"âœ… {label.upper()} -> Train: {len(train_files)}, "
                   f"Val: {len(val_files)}, Test: {len(test_files)}")

    return (
        split_dirs["train"]["real"],
        split_dirs["val"]["real"],
        split_dirs["test"]["real"]
    )


def create_augmentation_pipeline():
    """
    Data augmentation pipeline'Ä± oluÅŸturur.

    Returns:
        Augmentation fonksiyonu
    """
    import numpy as np
    from PIL import Image, ImageEnhance, ImageFilter

    def augment_image(
        image: Image.Image,
        rotate_range: Tuple[int, int] = (-10, 10),
        brightness_range: Tuple[float, float] = (0.9, 1.1),
        contrast_range: Tuple[float, float] = (0.9, 1.1),
        saturation_range: Tuple[float, float] = (0.9, 1.1),
        gaussian_blur: bool = False,
        p_blur: float = 0.3
    ) -> Image.Image:
        """
        GÃ¶rseli rastgele augment eder.

        Args:
            image: PIL Image
            rotate_range: DÃ¶ndÃ¼rme aÃ§Ä±sÄ± aralÄ±ÄŸÄ±
            brightness_range: ParlaklÄ±k aralÄ±ÄŸÄ±
            contrast_range: Kontrast aralÄ±ÄŸÄ±
            saturation_range: Doygunluk aralÄ±ÄŸÄ±
            gaussian_blur: Gaussian blur uygula
            p_blur: Blur olasÄ±lÄ±ÄŸÄ±

        Returns:
            Augment edilmiÅŸ gÃ¶rsel
        """
        img = image.copy()

        # Rastgele dÃ¶ndÃ¼rme
        if rotate_range:
            angle = random.uniform(*rotate_range)
            img = img.rotate(angle, expand=False, fillcolor=(255, 255, 255))

        # ParlaklÄ±k
        if brightness_range:
            factor = random.uniform(*brightness_range)
            enhancer = ImageEnhance.Brightness(img)
            img = enhancer.enhance(factor)

        # Kontrast
        if contrast_range:
            factor = random.uniform(*contrast_range)
            enhancer = ImageEnhance.Contrast(img)
            img = enhancer.enhance(factor)

        # Doygunluk
        if saturation_range:
            factor = random.uniform(*saturation_range)
            enhancer = ImageEnhance.Color(img)
            img = enhancer.enhance(factor)

        # Gaussian blur (opsiyonel)
        if gaussian_blur and random.random() < p_blur:
            img = img.filter(ImageFilter.GaussianBlur(radius=random.uniform(0.5, 1.5)))

        return img

    return augment_image


def apply_augmentation(
    input_dir: Path,
    output_dir: Path,
    augment_factor: int = 3
) -> Path:
    """
    Train setine data augmentation uygular.

    Args:
        input_dir: Train set dizini
        output_dir: ArtÄ±rÄ±lmÄ±ÅŸ veri Ã§Ä±ktÄ± dizini
        augment_factor: Her gÃ¶rsel iÃ§in kaÃ§ augmentation uygulanacaÄŸÄ±

    Returns:
        Ã‡Ä±ktÄ± dizini
    """
    from PIL import Image

    logger.info(f"ğŸ¨ Data augmentation uygulanÄ±yor (x{augment_factor})...")

    # Pipeline oluÅŸtur
    augment_fn = create_augmentation_pipeline()

    # Ã‡Ä±ktÄ± dizinini oluÅŸtur
    output_dir.mkdir(parents=True, exist_ok=True)

    # GÃ¶rselleri bul
    image_files = []
    for label in ["real", "fake"]:
        label_dir = input_dir / label
        if label_dir.exists():
            for ext in ["*.png", "*.jpg", "*.jpeg"]:
                image_files.extend(label_dir.glob(ext))

    logger.info(f"ğŸ“ {len(image_files)} gÃ¶rsel iÅŸlenecek")

    # Augmentation uygula
    processed = 0
    for img_path in image_files:
        try:
            img = Image.open(img_path)

            # Orijinali kopyala
            label_dir = output_dir / img_path.parent.name
            label_dir.mkdir(parents=True, exist_ok=True)

            # Orijinali kaydet
            img.save(label_dir / img_path.name, quality=95)

            # Augment edilmiÅŸ versiyonlarÄ± oluÅŸtur
            for i in range(augment_factor):
                aug_img = augment_fn(img)
                aug_name = f"{img_path.stem}_aug_{i}{img_path.suffix}"
                aug_img.save(label_dir / aug_name, quality=95)

            processed += 1

            if processed % 10 == 0:
                logger.info(f"  Ä°lerleme: {processed}/{len(image_files)}")

        except Exception as e:
            logger.warning(f"âš ï¸ GÃ¶rsel iÅŸlenemedi {img_path.name}: {e}")
            continue

    logger.info(f"âœ… Augmentation tamamlandÄ±: {processed} gÃ¶rsel")
    return output_dir


def create_dataset_info(
    input_dir: Path,
    output_path: Path
) -> None:
    """
    Veriseti hakkÄ±nda bilgi dosyasÄ± oluÅŸturur.

    Args:
        input_dir: Veriseti dizini
        output_path: Ã‡Ä±ktÄ± dosyasÄ±
    """
    from PIL import Image

    info = {
        "real": {"train": 0, "val": 0, "test": 0},
        "fake": {"train": 0, "val": 0, "test": 0}
    }

    for split in ["train", "val", "test"]:
        for label in ["real", "fake"]:
            label_dir = input_dir / split / label
            if label_dir.exists():
                count = len(list(label_dir.glob("*.png")) +
                          list(label_dir.glob("*.jpg")) +
                          list(label_dir.glob("*.jpeg")))
                info[label][split] = count

    # Dosyaya yaz
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("# AI Human Detector - Veri Seti Bilgisi\n\n")
        f.write("## BÃ¶lÃ¼m Ã–zeti\n\n")
        f.write("| Etiket | Train | Val | Test | Toplam |\n")
        f.write("|--------|-------|-----|------|-------|\n")

        for label in ["real", "fake"]:
            total = sum(info[label].values())
            f.write(f"| {label.upper()} | {info[label]['train']} | "
                   f"{info[label]['val']} | {info[label]['test']} | {total} |\n")

        f.write("\n## DetaylÄ± Bilgi\n\n")
        for split in ["train", "val", "test"]:
            f.write(f"### {split.upper()}\n\n")
            real_count = info["real"][split]
            fake_count = info["fake"][split]
            total = real_count + fake_count
            f.write(f"- Real: {real_count}\n")
            f.write(f"- Fake: {fake_count}\n")
            f.write(f"- Toplam: {total}\n\n")

    logger.info(f"âœ… Bilgi dosyasÄ± oluÅŸturuldu: {output_path}")


def main():
    parser = argparse.ArgumentParser(
        description="AI Human Detector - Veri Seti HazÄ±rlama Scripti"
    )
    parser.add_argument(
        "--input",
        type=str,
        default="./data/datasets",
        help="Girdi dizini (indirilen veriseti)"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="./data/processed",
        help="Ã‡Ä±ktÄ± dizini"
    )
    parser.add_argument(
        "--train-ratio",
        type=float,
        default=0.8,
        help="EÄŸitim oranÄ± (varsayÄ±lan: 0.8)"
    )
    parser.add_argument(
        "--val-ratio",
        type=float,
        default=0.1,
        help="DoÄŸrulama oranÄ± (varsayÄ±lan: 0.1)"
    )
    parser.add_argument(
        "--test-ratio",
        type=float,
        default=0.1,
        help="Test oranÄ± (varsayÄ±lan: 0.1)"
    )
    parser.add_argument(
        "--augment",
        action="store_true",
        help="Data augmentation uygula"
    )
    parser.add_argument(
        "--augment-factor",
        type=int,
        default=3,
        help="Augmentation Ã§arpanÄ± (varsayÄ±lan: 3)"
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="Rastgelelik tohumu"
    )

    args = parser.parse_args()

    # BaÄŸÄ±mlÄ±lÄ±klarÄ± kontrol et
    if not check_dependencies():
        sys.exit(1)

    try:
        input_dir = Path(args.input)
        output_dir = Path(args.output)

        # 1. Train/Val/Test bÃ¶l
        logger.info("ğŸ”„ ADIM 1: Veriseti bÃ¶lÃ¼nÃ¼yor...")
        split_dataset(
            input_dir=input_dir,
            output_dir=output_dir,
            train_ratio=args.train_ratio,
            val_ratio=args.val_ratio,
            test_ratio=args.test_ratio,
            seed=args.seed
        )

        # 2. Data augmentation (opsiyonel)
        if args.augment:
            logger.info("ğŸ”„ ADIM 2: Data augmentation uygulanÄ±yor...")
            train_dir = output_dir / "train"
            aug_dir = output_dir / "train_augmented"

            # Real gÃ¶rselleri augment et
            for label in ["real", "fake"]:
                label_input = train_dir / label
                if label_input.exists():
                    apply_augmentation(
                        input_dir=label_input,
                        output_dir=aug_dir / label,
                        augment_factor=args.augment_factor
                    )

        # 3. Bilgi dosyasÄ± oluÅŸtur
        logger.info("ğŸ”„ ADIM 3: Bilgi dosyasÄ± oluÅŸturuluyor...")
        create_dataset_info(
            input_dir=output_dir,
            output_path=output_dir / "DATASET_INFO.md"
        )

        logger.info("ğŸ‰ Veriseti hazÄ±rlama tamamlandÄ±!")

    except Exception as e:
        logger.error(f"âŒ Hata: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
